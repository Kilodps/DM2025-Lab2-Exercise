{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: Daniel Schulz\n",
    "\n",
    "Student ID: J144030003\n",
    "\n",
    "GitHub ID: 79483364\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "In this preprocessing phase, I focused on converting the raw, nested data sources into a structured tabular format suitable for analysis.\n",
    "\n",
    "First, I loaded the auxiliary CSV files containing data identifiers and emotion labels, alongside the primary content stored in a complex JSON format. Since the JSON structure was deeply nested, I iterated through the records to extract only the essential fields: the post_id, raw text, and hashtags.\n",
    "\n",
    "Simultaneously, I applied the NLTK library to perform initial word tokenization on the text, storing the tokenized lists as a separate feature. To correctly separate the dataset, I implemented logic to cross-reference each post's ID with the data_identification.csv file. This allowed me to sort entries into training or testing DataFrames dynamically. Finally, I merged the ground-truth emotion labels into the training set and exported both sets to CSV files for the next stage.\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "In this section, I focused on preparing the raw Twitter data for my model by cleaning inconsistencies and enriching the semantic content.\n",
    "\n",
    "First, I addressed data quality issues in the hashtags column. I noticed that some entries were valid Python lists, while others were string representations (e.g., '[]' or 'nan'). I implemented a cleaning function (clean_hashtags_for_join) to standardize all entries into proper lists, ensuring I could reliably access hashtag data.\n",
    "\n",
    "Next, I engineered a new input feature by merging the tweet text with its associated hashtags. I hypothesized that hashtags contain crucial context (e.g., sentiment or topic) that might be missing from the short tweet body. I used conditional logic to append these hashtags to the text only when they existed, avoiding unnecessary whitespace.\n",
    "\n",
    "Finally, I applied a text normalization pipeline (clean_tweet). To reduce noise, I replaced specific URLs and user mentions with generic tokens ([URL], [USER]) since the specific links or names are less relevant than their presence. I also integrated the emoji library to 'demojize' text, allowing my model to interpret the sentiment of icons as standard text.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "In the modeling phase, I adopted a hybrid approach, combining the power of modern Deep Learning with a robust traditional machine learning baseline to maximize classification accuracy.\n",
    "\n",
    "First, I implemented a Transformer model using vinai/bertweet-base. I selected this specific pre-trained model because it was trained on 850 million English tweets and is inherently better at understanding Twitter-specific nuances (like hashtags, handles, and slang) than standard BERT. I fine-tuned this model using the Hugging Face Trainer API for 3 epochs with a learning rate of 2e-5.\n",
    "\n",
    "Second, to create a stable baseline and capture linear patterns, I built a Logistic Regression pipeline. I used TfidfVectorizer to convert the text into numerical features (using both unigrams and bigrams) and trained the model with balanced class weights to handle any potential data imbalance.\n",
    "\n",
    "Finally, I implemented a Soft-Voting Ensemble to combine these models. Instead of relying on a single prediction, I extracted the probability scores (confidence levels) from both the BERTweet model and the Logistic Regression model. I calculated a weighted average of these probabilities, assigning a 70% weight to the Transformer (due to its higher complexity) and a 30% weight to the Logistic Regression. The final class prediction was determined by the highest score in this combined probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "I started by applying classical machine learning techniques to test their efficacy on classifying the raw text. The initial Naive Bayes approach was fast but underperformed with an accuracy of 0.4885. Logistic Regression offered a distinct improvement, raising the accuracy to 0.5964. Moving toward transformer architectures, I fine-tuned a standard BERT model which resulted in an accuracy of 0.6788, which was a significant leap that nearly matched my top results.\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "Working with various transformer architectures and ML techniques yielded two critical insights. First, I discovered that simply fine-tuning a language model on raw data is insufficient for optimal performance. Robust preprocessing and feature engineering proved to be pivotal, significantly influencing the model's final accuracy. This finding directly correlates with my second insight: when I experimented with different pre-trained BERT variants—including those specifically trained on Twitter data—I observed only negligible differences in their performance scores. Ultimately, the quality of data preparation and the structure of the input pipeline outweighed the specific choice of the pre-trained base model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "\n",
    "df_data_id = pd.read_csv(\"/kaggle/input/dm-lab-2-private-competition/data_identification.csv\")\n",
    "df_data_emotions = pd.read_csv(\"/kaggle/input/dm-lab-2-private-competition/emotion.csv\")\n",
    "\n",
    "df_data = pd.read_json(\"/kaggle/input/dm-lab-2-private-competition/final_posts.json\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "columns = ['post_id', 'text','tokenized','hashtags','label']\n",
    "test_df = pd.DataFrame(columns = columns)\n",
    "train_df = pd.DataFrame(columns = ['post_id', 'text', 'tokenized','hashtags'])\n",
    "length = len(df_data['root'])\n",
    "counter = 0\n",
    "\n",
    "for entry in df_data['root']:\n",
    "    post_id = entry[\"_source\"][\"post\"][\"post_id\"]\n",
    "    text = entry[\"_source\"][\"post\"][\"text\"]\n",
    "    tokenized = word_tokenize(entry[\"_source\"][\"post\"][\"text\"])\n",
    "    hashtags = entry[\"_source\"][\"post\"][\"hashtags\"]\n",
    "\n",
    "    #print(df_data_id[df_data_id[\"id\"] == post_id][\"split\"].values[0])\n",
    "\n",
    "    if df_data_id[df_data_id[\"id\"] == post_id][\"split\"].values[0] == \"train\":\n",
    "        emotion = df_data_emotions[df_data_emotions['id'] == post_id]['emotion']\n",
    "        entry_df = {'post_id': post_id, 'text' : text, 'tokenized' : tokenized,'hashtags' : hashtags,'label': emotion}\n",
    "        train_df.loc[len(train_df)] = entry_df\n",
    "    else:\n",
    "        entry_df = {'post_id': post_id, 'text' : text, 'tokenized' : tokenized,'hashtags' : hashtags}\n",
    "        test_df.loc[len(test_df)] = entry_df\n",
    "    counter += 1\n",
    "    if counter%1000 == 0:\n",
    "        print(\"Processed \", counter, \"/\", length, \" rows\")\n",
    "\n",
    "df_data_emotions.rename(columns={'id': 'post_id'}, inplace=True)\n",
    "train_df = train_df.merge(df_data_emotions[['post_id', 'emotion']], on='post_id', how='left')\n",
    "\n",
    "train_df.rename(columns={'emotion': 'label'}, inplace=True)\n",
    "\n",
    "train_df.to_csv(\"/kaggle/working/train.csv\")\n",
    "test_df.to_csv(\"/kaggle/working/test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "def combine_text_and_hashtags(row):\n",
    "    \"\"\"Combines text and hashtags, adding a space only if hashtags exist.\"\"\"\n",
    "    text = row['text']\n",
    "\n",
    "    if len(row['hashtags']) == 0 or len(row['hashtags']) == 1: \n",
    "        return text\n",
    "    \n",
    "    hashtags_str = ' '.join(row['hashtags']) # Joins the list\n",
    "\n",
    "    if hashtags_str:\n",
    "        # If hashtags exist, add the space for separation\n",
    "        return text + ' ' + hashtags_str\n",
    "    else:\n",
    "        # If the list was empty, return only the cleaned text\n",
    "        return text\n",
    "\n",
    "# Apply the function to create the new column\n",
    "train_df['input_text'] = train_df.apply(combine_text_and_hashtags, axis=1)\n",
    "\n",
    "def clean_hashtags_for_join(item):\n",
    "    \"\"\"\n",
    "    Standardizes the 'hashtags' column by converting string representations\n",
    "    of empty data (like '[]' or NaN) into a proper empty Python list [].\n",
    "    \"\"\"\n",
    "    # 1. Handle actual Python lists (return them as is)\n",
    "    if isinstance(item, list):\n",
    "        return item\n",
    "    \n",
    "    # 2. Handle missing/NaN values\n",
    "    if pd.isna(item):\n",
    "        return []\n",
    "    \n",
    "    # 3. CRITICAL: Handle string representations of empty lists/missing data\n",
    "    if isinstance(item, str):\n",
    "        cleaned_str = item.strip()\n",
    "        \n",
    "        # Check for common string forms of empty/missing data\n",
    "        if cleaned_str in ('[]', '[ ]', 'nan', ''):\n",
    "            return []\n",
    "        \n",
    "        # If it's a non-list string with actual content, treat it as a list with one item\n",
    "        return [cleaned_str] \n",
    "        \n",
    "    # 4. Fallback for other unexpected types\n",
    "    return [] \n",
    "\n",
    "# Apply the cleaning to your column\n",
    "train_df['hashtags_clean'] = train_df['hashtags'].apply(clean_hashtags_for_join)\n",
    "\n",
    "# Create the joined string column (will be '' for empty lists)\n",
    "train_df['hashtags_str'] = train_df['hashtags_clean'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Concatenate only if the hashtag string is NOT empty\n",
    "def conditional_concat(row):\n",
    "    text = row['text_cleaned']\n",
    "    hashtags = row['hashtags_str']\n",
    "    \n",
    "    if hashtags:\n",
    "        # If hashtags_str has content, add the space and the hashtags\n",
    "        return text + ' ' + hashtags\n",
    "    else:\n",
    "        # If hashtags_str is '', just return the text\n",
    "        return text\n",
    "\n",
    "train_df['input_text'] = train_df.apply(conditional_concat, axis=1)\n",
    "\n",
    "# Final safety strip to remove any accidental leading/trailing spaces\n",
    "train_df['input_text'] = train_df['input_text'].str.strip()\n",
    "\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Replace URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '[URL]', text, flags=re.MULTILINE)\n",
    "    # Replace user mentions\n",
    "    text = re.sub(r'@\\w+', '[USER]', text)\n",
    "    # Split hashtags\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    return text\n",
    "\n",
    "# Apply this along with the emoji conversion\n",
    "train_df['input_text'] = train_df['text'].apply(clean_tweet)\n",
    "test_df['input_text'] = test_df['text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#train_df['hashtags_str'] = train_df['hashtags'].apply(lambda x: ' '.join(x))\n",
    "#train_df['input_text'] = train_df['text'] + ' ' + train_df['hashtags_str']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['labels'] = label_encoder.fit_transform(train_df['label'])\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "\n",
    "train_split_df = train_df.copy()\n",
    "#train_split_df, eval_split_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_split_df.reset_index(drop=True, inplace=True)\n",
    "#eval_split_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Train DF Index:\", train_split_df.index)\n",
    "#print(\"Eval DF Index:\", eval_split_df.index)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_split_df)\n",
    "#eval_dataset = Dataset.from_pandas(eval_split_df)\n",
    "\n",
    "#train_dataset = train_dataset.remove_columns([\"label\", \"text\"])\n",
    "#eval_dataset = eval_dataset.remove_columns([\"label\", \"text\"])\n",
    "train_dataset = train_dataset.remove_columns([\"label\"])\n",
    "#eval_dataset = eval_dataset.remove_columns([\"label\"])\n",
    "\n",
    "#train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "#eval_dataset = eval_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "model_name = 'vinai/bertweet-base' #\"bert-base-uncased\"  # or \"roberta-base\", \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['text_cleaned'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "#eval_dataset = eval_dataset.map(preprocess, batched=True)\n",
    "\n",
    "\n",
    "#Training \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "num_labels = len(train_df['label'].unique())\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Training the Logistic Regression Pipeline\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = train_df[\"text_cleaned\"]\n",
    "y_train = train_df[\"labels\"]\n",
    "X_test = test_df[\"text_cleaned\"]\n",
    "\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), min_df=2, sublinear_tf=True)),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', solver='liblinear', C=0.5))\n",
    "])\n",
    "\n",
    "print(\"Training Logistic Regression pipeline...\")\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "final_predictions = logreg_pipeline.predict(X_test)\n",
    "print(f\"\\nPredicted classes: {final_predictions}\")\n",
    "\n",
    "\n",
    "logreg_probs = logreg_pipeline.predict_proba(X_test)\n",
    "print(f\"\\nPredicted probabilities (LogReg):\\n{logreg_probs}\")\n",
    "\n",
    "#Getting the transformer prediction \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- 1. Load your new test data ---\n",
    "# Make sure it has a 'text' column.\n",
    "test_df = pd.read_csv('/kaggle/working/test_cleaned.csv')\n",
    "\n",
    "# Convert to a Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "# --- 2. Tokenize the test data (using the same function as before) ---\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text_cleaned'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "# --- 3. Remove unnecessary columns and set format for PyTorch ---\n",
    "# The Trainer only needs the model's input columns.\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\"])\n",
    "tokenized_test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "\n",
    "# --- 4. Get predictions from the Trainer ---\n",
    "# This runs the model on your entire test set.\n",
    "raw_predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "\n",
    "# --- 5. Process the predictions to get the final labels ---\n",
    "# The raw_predictions object contains the logits (raw scores)\n",
    "logits = raw_predictions.predictions\n",
    "\n",
    "# Find the index of the highest score for each prediction\n",
    "predicted_label_ids = np.argmax(logits, axis=1)\n",
    "\n",
    "# Use the id2label mapping to convert the integer IDs back to string labels\n",
    "# (This assumes you created id2label during training, like in our previous steps)\n",
    "# id2label = {0: 'fear', 1: 'joy', ...}\n",
    "predicted_labels = [model.config.id2label[id] for id in predicted_label_ids]\n",
    "\n",
    "\n",
    "# --- You now have your list of predicted labels! ---\n",
    "print(\"Prediction for the first 10 tweets:\")\n",
    "for text, label in zip(test_df['text'][:10], predicted_labels[:10]):\n",
    "    print(f\"Tweet: {text}\\nPredicted Emotion: {label}\\n---\")\n",
    "\n",
    "# You can also add the predictions back to your DataFrame\n",
    "test_df['predicted_emotion'] = predicted_labels\n",
    "test_df.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "\n",
    "# Ensemble of Transformer and Logistic Regression\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "transformer_probs = softmax(logits, axis=1)\n",
    "\n",
    "print(f\"--- Ensembling Example ---\")\n",
    "print(f\"Transformer Probs:\\n{transformer_probs}\")\n",
    "\n",
    "bert_weight = 0.70\n",
    "logreg_weight = 0.30\n",
    "\n",
    "# This is the \"soft vote\"\n",
    "ensembled_probs = (bert_weight * transformer_probs) + (logreg_weight * logreg_probs)\n",
    "\n",
    "print(f\"\\nEnsembled (Averaged) Probs:\\n{ensembled_probs}\")\n",
    "\n",
    "\n",
    "# --- 4. Get the Final Prediction ---\n",
    "# Take the argmax of the averaged probabilities to get the final class\n",
    "final_ensemble_predictions = np.argmax(ensembled_probs, axis=1)\n",
    "\n",
    "print(f\"\\n--- FINAL PREDICTIONS ---\")\n",
    "print(f\"Transformer only: {np.argmax(transformer_probs, axis=1)}\")\n",
    "print(f\"LogReg only:      {np.argmax(logreg_probs, axis=1)}\")\n",
    "print(f\"Ensembled:        {final_ensemble_predictions}\")\n",
    "\n",
    "# Saving output \n",
    "\n",
    "predicted_labels = [model.config.id2label[id] for id in final_ensemble_predictions]\n",
    "new_df = pd.DataFrame()\n",
    "new_df['id'] = test_df[\"post_id\"]\n",
    "new_df[\"emotion\"] = predicted_labels\n",
    "new_df.rename(columns={\"post_id\": \"id\"}, inplace=True)\n",
    "new_df.to_csv(\"/kaggle/working/submission_logRegBertEns.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
